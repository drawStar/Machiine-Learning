{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCc94Qc5uO7E",
        "colab_type": "text"
      },
      "source": [
        "# **pytorch 实现 skip-gram**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqDktsI-MRDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import scipy\n",
        "\n",
        "# 固定seed\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1)\n",
        "\n",
        "# 设定超参数\n",
        "C = 3 #context window size\n",
        "K = 2 #number of negative samples \n",
        "NUM_EPOCHS = 1 #运行太慢了，这里设为1\n",
        "VOCAB_SIZE = 30000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.1\n",
        "EMBEDDING_SIZE = 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofJbClUBOcnR",
        "colab_type": "text"
      },
      "source": [
        "从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
        "\n",
        "由于单词数量可能太大，我们只选取最常见的VOCAB_SIZE个单词\n",
        "\n",
        "我们添加一个UNK单词表示所有不常见的单词\n",
        "\n",
        "需记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JTfybzUOhdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"text8.train.txt\",\"r\") as fin:\n",
        "    text = fin.read()\n",
        "\n",
        "text = text.split() #分词，以最简单的空格进行分词\n",
        "\n",
        "# 构建词汇表\n",
        "\n",
        "#Counter会统计每个单词的次数\n",
        "#mostcommon（m）找出出现次数最多的m个单词作为词库，-1是因为要分配给UNK一个\n",
        "vocab = dict(Counter(text).most_common(VOCAB_SIZE-1))\n",
        "#剩下的词看作不常出现的单词，计算这些单词次数(所有单词出现的次数-词库中每个单词出现的次数的和)\n",
        "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
        "#vocab中记录了每个单词及其出现的次数\n",
        "\n",
        "idx_to_word = {index:word for index,word in enumerate(vocab)}\n",
        "word_to_idx = {word:index for index,word in idx_to_word.items()}\n",
        "# print(list(idx_to_word.items())[:10])\n",
        "# print(list(word_to_idx.items())[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmqAi6Q4bAxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 计算word frequency以便用于负采样\n",
        "word_counts = np.array([count for count in vocab.values()],dtype=np.float32) #m每个单词出现的频次count\n",
        "word_frequencys = word_counts / np.sum(word_counts) #每个单词出现的频率 \n",
        "word_frequencys = word_frequencys **(3.0/4.0) #论文中提到的3/4\n",
        "word_frequencys = word_frequencys / np.sum(word_frequencys) #实际上是记录了每个单词被采样的权重（可看作概率）\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRfYOleKs4qJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 实现Dataloader\n",
        "\n",
        "一个dataloader需要以下内容：\n",
        "\n",
        "把所有text编码成数字\n",
        "\n",
        "保存vocabulary，单词count，normalized word frequency\n",
        "\n",
        "每个iteration sample一个中心词\n",
        "\n",
        "根据当前的中心词返回context单词\n",
        "\n",
        "根据中心词sample一些negative单词\n",
        "\n",
        "\n",
        "这里有一个好的tutorial介绍如何使用PyTorch dataloader. 为了使用dataloader，我们需要定义以下两个function:\n",
        "\n",
        "$$__len__(self)需要返回整个数据集中有多少个item$$\n",
        "\n",
        "$$__getitem__(self,index) 根据给定的index返回一个item$$\n",
        "\n",
        "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-7ecutecNC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.utils.data as tud\n",
        "\n",
        "class WordEmbeddingDataset(tud.Dataset):\n",
        "    def __init__(self,text,word_to_idx,idx_to_word,word_frequencys,word_counts):\n",
        "        #要记录这些信息\n",
        "        super(WordEmbeddingDataset,self).__init__()\n",
        "        #将woed用数字表示\n",
        "        self.text_encoded = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in text]\n",
        "        #dict.get(key, default=None),返回key对应的value，如哦value不存在，返回default设定的值\n",
        "\n",
        "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_frequencys = torch.Tensor(word_frequencys)\n",
        "        self.word_counts = torch.Tensor(word_counts)\n",
        "    def __len__(self):\n",
        "        #返回这个数据集一共多少个item\n",
        "        return len(self.text_encoded)\n",
        "    def __getitem__(self,idx):\n",
        "        #通过__getitem__函数获取单个的数据，然后组合成batch\n",
        "        center_word = self.text_encoded[idx] # 中心词的id\n",
        "        pos_indices = list(range(idx-C,idx)) + list(range(idx+1,idx+C+1))#中心词的前c个和后c个\n",
        "        #处理边界情况，idx-C可能<0 or idx+C>len\n",
        "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
        "        pos_words = self.text_encoded[pos_indices]#hang list[list]?答lsit[list]不支持，tensor[lsit]支持\n",
        "\n",
        "        #negative words\n",
        "        #multinomial 多项式分布概率采样(input, num_samples, replacement=False)\n",
        "        #input张量可以看成一个权重张量，每一个元素代表其在该行中的权重\n",
        "        #如果有元素为0，那么在其他不为0的元素，被取干净之前，这个元素是不会被取到的。\n",
        "        #返回其索引，若input有m行，则抽取m乘numsamples个，True表示有放回的采样\n",
        "        neg_words = torch.multinomial(self.word_frequencys, K*pos_words.shape[0], False)\n",
        "        # hang freq？不应该是正样本以外的样本中抽取吗，对每个上下文词都要抽取K个负样本\n",
        "        #答：multinomial是根据权重抽取其对应的索引\n",
        "\n",
        "        return center_word, pos_words, neg_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-5V6sgKBbiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = WordEmbeddingDataset(text,word_to_idx,idx_to_word,word_frequencys,word_counts)\n",
        "dataloader = tud.DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)#numworkers好像是指以4个线程启动"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSU_5AQDKewp",
        "colab_type": "text"
      },
      "source": [
        "## 定义pytoch模型\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLBNtLfMI9mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(EmbeddingModel, self).__init__()\n",
        "        initrange = 0.5 / embed_dim #可选\n",
        "        self.in_embed = nn.Embedding(vocab_size,embed_dim)\n",
        "        self.in_embed.weight.data.uniform_(-initrange,initrange) #可选\n",
        "        self.out_embed = nn.Embedding(vocab_size,embed_dim)\n",
        "        \n",
        "    def forward(self,input_labels,pos_labels,neg_labels):\n",
        "        \"\"\"\n",
        "        input_labels: 输入的中心词, [batch_size]，就是单词的id\n",
        "        pos_labels: 中心词周围 的单词 [batch_size，(window_size*2)]每个中心词对应windowsszie*2个正样本词\n",
        "        neg_labelss: [batch_size, (window_size*2*K)]#对每个pos词，都要采K个负样本\n",
        "        \n",
        "        return: loss, [batch_size]\n",
        "        \"\"\"\n",
        "        #embedding层的输入是整数索引，即单词的index\n",
        "        input_embedding = self.in_embed(input_labels) #【batchsize, embeddim】\n",
        "        pos_embedding = self.in_embed(pos_labels)#【batch_size，(window_size*2)，embeddim】\n",
        "        neg_embedding = self.in_embed(neg_labels)#【batch_size, (window_size*2*K)，embeddim】\n",
        "\n",
        "        #需要将inputembedding和posembedding做点积\n",
        "        input_embedding = input_embedding.unsqueeze(2)#在第w维加上1维，【batchsize, embeddim，1】\n",
        "\n",
        "        pos_dot = torch.bmm(pos_embedding,input_embedding)#【batch_size，(window_size*2)，1】\n",
        "        #input是【b，n，m】tensor, mat2是【b，m，p】tensor,torch.bmm（input,mat2）得到【b，n，p】tensor.\n",
        "\n",
        "        pos_dot = pos_dot.squeeze(2)#去掉第2维的1，【batch_size，(window_size*2)】\n",
        "\n",
        "        neg_dot = torch.bmm(neg_embedding,input_embedding)#【batch_size，(window_size*2*K)，1】\n",
        "        neg_dot = neg_dot.squeeze(2)#去掉第2维的1，【batch_size，(window_size*2*K)】\n",
        "\n",
        "        log_pos = F.logsigmoid(pos_dot).sum(1)#第一维是batch\n",
        "        log_neg = F.logsigmoid(-neg_dot).sum(1) #hang 关于logsigmoid和n-neg_dot见笔记，作者这里没有加负号\n",
        "\n",
        "        loss = log_pos + log_neg\n",
        "        \n",
        "        return -loss #求argmin.【batchsize】\n",
        "    \n",
        "    def input_embedding(self):#获取embedding\n",
        "        return self.in_embed.weight.data.cpu().numpy()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arm2_uF0ToQW",
        "colab_type": "text"
      },
      "source": [
        "**定义模型以及将模型移动到GPU**\n",
        "\n",
        "每个epoch我们都把所有的数据分成若干个batch\n",
        "\n",
        "把每个batch的输入和输出都包装成cuda tensor\n",
        "\n",
        "forward pass，通过输入的句子预测每个单词的下一个单词\n",
        "\n",
        "用模型的预测和正确的下一个单词计算cross entropy loss\n",
        "\n",
        "清空模型当前gradient\n",
        "\n",
        "backward pass\n",
        "\n",
        "更新模型参数\n",
        "\n",
        "每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WjSjxqQNaNQ",
        "colab_type": "code",
        "outputId": "6bc34515-fc2d-4f50-be63-2548805c0f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "#lossfn = #这里不需要了，因为已经在model的forward定义了loss\n",
        "#hang  那loss.backward（）怎么办？\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for epochIndex in range(NUM_EPOCHS):\n",
        "    print(\"epoch:\",epochIndex)\n",
        "    for i,(input_labels,pos_labels,neg_labels) in enumerate(dataloader):#对每个batchsize\n",
        "\n",
        "        input_labels = input_labels.long()\n",
        "        pos_labels = pos_labels.long()\n",
        "        neg_labels = neg_labels.long()\n",
        "        if torch.cuda.is_available():\n",
        "            input_labels = input_labels.cuda()\n",
        "            pos_labels = pos_labels.cuda()\n",
        "            neg_labels = neg_labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(input_labels,pos_labels,neg_labels).mean() #model.forward()\n",
        "            #注意loss应该是a tensor with single number，我们的是【batchsize】大小的tensor，所以做个mean\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(\"batch:\",i,loss.item())\n",
        "    break            \n",
        "    if epochIndex %10 == 0:\n",
        "        print(\"epoch:\",epochIndex, loss.item())\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "batch: 0 12.476619720458984\n",
            "batch: 1000 12.44610595703125\n",
            "batch: 2000 12.291523933410645\n",
            "batch: 3000 12.357091903686523\n",
            "batch: 4000 12.278715133666992\n",
            "batch: 5000 12.131007194519043\n",
            "batch: 6000 12.270715713500977\n",
            "batch: 7000 12.223976135253906\n",
            "batch: 8000 12.267290115356445\n",
            "batch: 9000 12.310798645019531\n",
            "batch: 10000 12.184684753417969\n",
            "batch: 11000 12.246097564697266\n",
            "batch: 12000 12.23458194732666\n",
            "batch: 13000 12.22145938873291\n",
            "batch: 14000 12.235976219177246\n",
            "batch: 15000 12.13444995880127\n",
            "batch: 16000 12.193893432617188\n",
            "batch: 17000 12.246011734008789\n",
            "batch: 18000 12.132369995117188\n",
            "batch: 19000 12.228584289550781\n",
            "batch: 20000 12.10348892211914\n",
            "batch: 21000 12.113518714904785\n",
            "batch: 22000 12.120660781860352\n",
            "batch: 23000 12.272673606872559\n",
            "batch: 24000 12.20083236694336\n",
            "batch: 25000 12.30928897857666\n",
            "batch: 26000 12.189101219177246\n",
            "batch: 27000 12.185226440429688\n",
            "batch: 28000 12.111093521118164\n",
            "batch: 29000 12.210922241210938\n",
            "batch: 30000 12.121561050415039\n",
            "batch: 31000 12.182662010192871\n",
            "batch: 32000 12.181356430053711\n",
            "batch: 33000 12.137855529785156\n",
            "batch: 34000 12.12141227722168\n",
            "batch: 35000 12.190162658691406\n",
            "batch: 36000 12.187877655029297\n",
            "batch: 37000 12.041340827941895\n",
            "batch: 38000 12.090699195861816\n",
            "batch: 39000 12.172012329101562\n",
            "batch: 40000 12.26092529296875\n",
            "batch: 41000 12.210103988647461\n",
            "batch: 42000 12.221586227416992\n",
            "batch: 43000 12.138338088989258\n",
            "batch: 44000 12.210220336914062\n",
            "batch: 45000 12.180197715759277\n",
            "batch: 46000 12.258272171020508\n",
            "batch: 47000 12.04957103729248\n",
            "batch: 48000 12.165949821472168\n",
            "batch: 49000 12.1692476272583\n",
            "batch: 50000 12.194726943969727\n",
            "batch: 51000 12.108537673950195\n",
            "batch: 52000 12.139599800109863\n",
            "batch: 53000 12.159523963928223\n",
            "batch: 54000 12.24868106842041\n",
            "batch: 55000 12.127941131591797\n",
            "batch: 56000 12.109172821044922\n",
            "batch: 57000 12.205765724182129\n",
            "batch: 58000 12.099699020385742\n",
            "batch: 59000 12.1954345703125\n",
            "batch: 60000 12.111817359924316\n",
            "batch: 61000 12.051593780517578\n",
            "batch: 62000 12.100160598754883\n",
            "batch: 63000 12.011338233947754\n",
            "batch: 64000 12.183150291442871\n",
            "batch: 65000 12.234758377075195\n",
            "batch: 66000 12.08741569519043\n",
            "batch: 67000 11.926416397094727\n",
            "batch: 68000 12.20372200012207\n",
            "batch: 69000 11.990662574768066\n",
            "batch: 70000 12.166322708129883\n",
            "batch: 71000 12.020898818969727\n",
            "batch: 72000 12.160215377807617\n",
            "batch: 73000 12.095207214355469\n",
            "batch: 74000 12.12101936340332\n",
            "batch: 75000 12.037056922912598\n",
            "batch: 76000 12.04686450958252\n",
            "batch: 77000 12.080047607421875\n",
            "batch: 78000 12.137632369995117\n",
            "batch: 79000 12.078720092773438\n",
            "batch: 80000 11.974603652954102\n",
            "batch: 81000 12.000460624694824\n",
            "batch: 82000 12.00971508026123\n",
            "batch: 83000 12.095197677612305\n",
            "batch: 84000 12.0726318359375\n",
            "batch: 85000 12.14439582824707\n",
            "batch: 86000 12.26003646850586\n",
            "batch: 87000 12.204248428344727\n",
            "batch: 88000 12.077408790588379\n",
            "batch: 89000 12.038710594177246\n",
            "batch: 90000 12.088775634765625\n",
            "batch: 91000 12.145489692687988\n",
            "batch: 92000 12.125678062438965\n",
            "batch: 93000 12.134435653686523\n",
            "batch: 94000 12.147565841674805\n",
            "batch: 95000 12.060569763183594\n",
            "batch: 96000 12.03228759765625\n",
            "batch: 97000 12.186664581298828\n",
            "batch: 98000 12.001862525939941\n",
            "batch: 99000 12.073541641235352\n",
            "batch: 100000 12.292724609375\n",
            "batch: 101000 12.118144035339355\n",
            "batch: 102000 12.104246139526367\n",
            "batch: 103000 12.201578140258789\n",
            "batch: 104000 12.017744064331055\n",
            "batch: 105000 12.235910415649414\n",
            "batch: 106000 12.066244125366211\n",
            "batch: 107000 11.9564208984375\n",
            "batch: 108000 11.967561721801758\n",
            "batch: 109000 12.000818252563477\n",
            "batch: 110000 12.010919570922852\n",
            "batch: 111000 12.005044937133789\n",
            "batch: 112000 12.089584350585938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-a445bc39020f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepochIndex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#对每个batchsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minput_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaIWWxA5ntjQ",
        "colab_type": "text"
      },
      "source": [
        "# 评估模型\n",
        "\n",
        " - 计算斯皮尔曼系数\n",
        "\n",
        " - find最近的单词"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4C4dIWXNalh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "a1d3bc8a-037d-4320-a5ab-a7f760f66f9d"
      },
      "source": [
        "import scipy.spatial\n",
        "\n",
        "#获得所有单词的embedding\n",
        "embedding_weights = model.input_embedding()\n",
        "\n",
        "def find_nearest(word):\n",
        "    #与所有单词的embedding向量计算余弦距离，返回最小的5个单词\n",
        "    word_id = word_to_idx[word]\n",
        "    embedding = embedding_weights[word_id]\n",
        "    cos_distance = np.array([scipy.spatial.distance.cosine(e,embedding) for e in embedding_weights])\n",
        "    return [idx_to_word[i] for i in cos_distance.argsort()[:5]]\n",
        "\n",
        "for word in [\"good\", \"green\", \"like\", \"work\", \"computer\"]:\n",
        "    print(word, find_nearest(word))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good ['good', 'simple', 'technique', 'very', 'device']\n",
            "green ['green', 'blue', 'etc', 'binomial', 'rice']\n",
            "like ['like', 'plural', 'disease', 'singular', 'extinct']\n",
            "work ['work', 'his', 'her', 'life', 'home']\n",
            "computer ['computer', 'natural', 'analysis', 'variant', 'discussion']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3HkQvvhq2mv",
        "colab_type": "text"
      },
      "source": [
        "## 单词之间的关系\n",
        "\n",
        "man-king = women-queen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvukFSX8qLUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "f3a5ad44-0766-4a2c-a2ad-9b868057ae8f"
      },
      "source": [
        "\n",
        "man_idx = word_to_idx[\"man\"] \n",
        "king_idx = word_to_idx[\"king\"] \n",
        "woman_idx = word_to_idx[\"woman\"]\n",
        "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
        "#在所有单词的embedding中找出与这个embedding最近的单词\n",
        "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
        "for i in cos_dis.argsort()[:10]:\n",
        "    print(idx_to_word[i])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "king\n",
            "emperor\n",
            "president\n",
            "st\n",
            "queen\n",
            "duke\n",
            "prince\n",
            "alexander\n",
            "henry\n",
            "ii\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GOKUDw6sAzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}