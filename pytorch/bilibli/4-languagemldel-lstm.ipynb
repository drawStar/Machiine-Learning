{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCc94Qc5uO7E",
        "colab_type": "text"
      },
      "source": [
        "# 语言模型\n",
        "\n",
        "$$P(w1,w2,..wn) = P(w1)·P(w2|w1)·P(w3|w1,w2)·...·P(wn|w1,w2,...w_{n-1})$$\n",
        "\n",
        "学习语言模型，以及如何训练一个语言模型\n",
        "\n",
        "学习torchtext的基本使用方法\n",
        "\n",
        "构建 vocabulary， word to index 和 index to word\n",
        "\n",
        "学习torch.nn的一些基本模型\n",
        "Linear RNN LSTM GRU\n",
        "\n",
        "RNN的训练技巧\n",
        "\n",
        "Gradient Clipping\n",
        "\n",
        "如何保存和读取模型\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_18B9ixEnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
        "\n",
        "random.seed(53113)\n",
        "np.random.seed(53113)\n",
        "torch.manual_seed(53113)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(53113)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 25\n",
        "MAX_VOCAB_SIZE = 30000 #src=500000\n",
        "HIDDEN_SIZE = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2y1Iuygxm10",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "我们会继续使用上次的text8作为我们的训练，验证和测试数据\n",
        "\n",
        "TorchText的一个重要概念是Field，它决定了你的数据会如何被处理。我们使用TEXT这个field来处理文本数据。我们的TEXT field有lower=True这个参数，所以所有的单词都会被lowercase。\n",
        "\n",
        "torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集。\n",
        "\n",
        "build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量。\n",
        "\n",
        "BPTTIterator可以连续地得到连贯的句子，BPTT的全程是back propagation through time。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmcBL_RuxrOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = torchtext.data.Field(lower=True) #定义Field，名称为TEXT，用于预处理\n",
        "\n",
        "#划分数据集，path=\".\"表示当前文件夹\n",
        "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(path=\".\", \n",
        "    train=\"text8.train.txt\", validation=\"text8.dev.txt\", test=\"text8.test.txt\", text_field=TEXT)\n",
        "\n",
        "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE) #创建大小为MAX_VOCAB_SIZE的词库，\n",
        "#实际上大小是MAX_VOCAB_SIZE+2，TorchText会增加了两个特殊的token，<unk>表示未知的单词，<pad>表示padding。\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cunxB__BZqW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81a55456-ab58-4aa8-8033-39ac97fb2b1f"
      },
      "source": [
        "#pass\n",
        "len(TEXT.vocab) #50002\n",
        "TEXT.vocab.itos[:10] #indextostring ['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']\n",
        "TEXT.vocab.stoi[\"apple\"] #stringtoindex 1273\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1273"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh1k1ESCbWkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "# 定义迭代器\n",
        "train_iter,val_iter,test_iter = torchtext.data.BPTTIterator.splits((train,val,test),batch_size=BATCH_SIZE, device=device, bptt_len=50,repeat=False,shuffle=True)\n",
        "\n",
        "#bptt_len随时间反向传播的序列长度.hang seqLengt句子长度？"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TigJr03chh42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "46bfd32b-30d6-4090-89b6-31e723a4f6ba"
      },
      "source": [
        "#pass\n",
        "it = iter(train_iter)\n",
        "batch = next(it)\n",
        "batch #包含text和target，维度是【seqlength，batchsize】\n",
        "#---batch\n",
        "#[torchtext.data.batch.Batch of size 32]\n",
        "#\t[.text]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]\n",
        "#\t[.target]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]\n",
        "\n",
        "#取第一维度的数据看下，用前面的单词去预测后一个单词\n",
        "print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,0].data.cpu()))\n",
        "print()\n",
        "print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:,0].data.cpu()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the\n",
            "\n",
            "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTdcI84aoHYz",
        "colab_type": "text"
      },
      "source": [
        "## 定义模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFqbIJGhhqYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self,rnn_type,vocab_size,embed_size,hidden_size):\n",
        "        #rnn_type:model类型，RNN，LSTM，GRU\n",
        "        super(RNNModel,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed = nn.Embedding(vocab_size,embed_size) #输出【batchsize，embeddingdim】\n",
        "        self.lstm = nn.LSTM(embed_size,hidden_size,batch_first=False) #TODO\n",
        "        self.linear = nn.Linear(hidden_size,vocab_size) #计算\\haty,\n",
        "    def forward(self,text,hidden):\n",
        "        #text大小是【seqlength，batchsize】\n",
        "        emb = self.embed(text) #输出是【seqlength，batchsize，embeddingdim】\n",
        "        output, hidden = self.lstm(emb,hidden)\n",
        "        #output输出大小【seq_len, batchsize, num_directions*hidden_size】\n",
        "        #hidden输出大小(num_layers*num_directions, batch, hidden_size)\n",
        "        \n",
        "        \"\"\"\n",
        "        #因为线性变换的输入维度是2，而outpit的输出维度是3，要view成为2维的\n",
        "        output = output.view(-1,outpit.shape[2]) # 输出是【seqlen*batchsize，hiddensize】\n",
        "        out_vocab = self.linear(output) # 输出是【seqlenth*batchsize，vocabsize】\n",
        "\n",
        "        #变成【seqlen，batchsize，vocabsize】\n",
        "        out_vocab = out_vocab.view(output.shape[0],output.shape[1],out_vocab.shape[-1])\n",
        "        #TODO：why要变回来\n",
        "        \"\"\"\n",
        "        #nn.Linear()线性变化只针对最后一维，只要保证最后一维是输入就行\n",
        "        out_vocab = self.linear(output) # 输出是【seqlenth,batchsize，vocabsize】\n",
        "        \n",
        "        #TODO:why 不加softmax 答：lossfn是CrossEntropyLoss\n",
        "\n",
        "        return out_vocab, hidden\n",
        "    \n",
        "    def init_hidden(self,batchsize,requires_grad=True):\n",
        "        #TODO:\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros((1,batchsize,self.hidden_size),requires_grad=True),\n",
        "            weight.new_zeros((1,batchsize,self.hidden_size),requires_grad=True))\n",
        "        # 返回hiddenstate初始状态h0和 cellstate初始状态c0\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwF6z8rboPDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNNModel(rnn_type=\"LSTM\",\n",
        "                 vocab_size=len(TEXT.vocab),\n",
        "                 embed_size=EMBEDDING_SIZE,\n",
        "                 hidden_size=HIDDEN_SIZE)\n",
        "if USE_CUDA:\n",
        "    model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09-ax6yJD8rY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "9fbc5598-db15-47c5-f77d-5459053bcb60"
      },
      "source": [
        "#pass\n",
        "model"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (embed): Embedding(30002, 25)\n",
              "  (lstm): LSTM(25, 50)\n",
              "  (linear): Linear(in_features=50, out_features=30002, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz7HzlToHSMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    #为了防止内存爆炸，将hidden的图节点断掉，只保留值，不保留计算图的信息\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    total_count = 0.\n",
        "    with torch.no_grad():\n",
        "        it = iter(data)\n",
        "        hidden = model.init_hidden(BATCH_SIZE,requires_grad=False)\n",
        "        #注意：只有在语言模型中，不同batch之间是连续的（一篇文章分成了不同batch），\n",
        "        #所以hidden适合一直传下去，但内存会爆炸\n",
        "        for batchIndex,batch in enumerate(it):\n",
        "            data, target = batch.text, batch.target\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "            #croosentropy loss，希望haty是【样本数，vocabsize】，target是【样本数】，\n",
        "            #原haty【seqlenth,batchsize，vocabsize】，原traget【seqlength，batchsize】，所以要view下\n",
        "            loss = lossfn(output.view(-1,len(TEXT.vocab)),target.view(-1))\n",
        "             \n",
        "            #*表示解包，得到seqlength和batchszie，相乘得到样本数\n",
        "            total_count = np.multiply(*data.size())\n",
        "            total_loss = loss.item()*total_count#crossentropyloss默认返回均值\n",
        "    model.train() #val集合上完了，继续再train集合上训练\n",
        "    return total_loss /total_count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vutOGq_KrCCf",
        "colab_type": "text"
      },
      "source": [
        "## train and save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezLqQcn0FacV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lossfn = nn.CrossEntropyLoss()\n",
        "learning_rate=0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5) #lr decay\n",
        "GRAD_CLIP = 5.0\n",
        "NUM_EPOCH = 5\n",
        "valbestloss = 9999\n",
        "\n",
        "for epochIndex in range(NUM_EPOCH):\n",
        "    model.train()\n",
        "    it = iter(train_iter)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "    #注意：只有在语言模型中，不同batch之间是连续的（一篇文章分成了不同batch），\n",
        "    #所以hidden适合一直传下去，但内存会爆炸\n",
        "    for batchIndex,batch in enumerate(it):\n",
        "        data, target = batch.text, batch.target\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "        #croosentropy loss，希望haty是【样本数，vocabsize】，target是【样本数】，\n",
        "        #原haty【seqlenth,batchsize，vocabsize】，原traget【seqlength，batchsize】，所以要view下\n",
        "        loss = lossfn(output.view(-1,len(TEXT.vocab)),target.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        if batchIndex %1000:#每1000batch再验证集上计算valloss\n",
        "            valloss = evaluate(model, val_iter)\n",
        "            if valloss < valbestloss:\n",
        "                valbestloss = valloss\n",
        "                torch.save(model.state_dict(), \"lm.pth\") #save模型的参数，文件名为lm.pth\n",
        "                print(\"model saved\")\n",
        "            else: #每次loss不降就降lr，也可以连续n次不下降，这里为了简单\n",
        "                scheduler.step() #lr decay\n",
        "\n",
        "            print(\"epochIndex:\",epochIndex, loss.item(),valloss)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m98ZAcMoYoZ",
        "colab_type": "text"
      },
      "source": [
        "## 加载saved模型\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrLimAJcFa2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c13bf3b4-06df-44e5-8822-f779aea1422b"
      },
      "source": [
        "#先初始化模型，然后加载参数\n",
        "mysavedmodel = RNNModel(rnn_type=\"LSTM\",\n",
        "                 vocab_size=len(TEXT.vocab),\n",
        "                 embed_size=EMBEDDING_SIZE,\n",
        "                 hidden_size=HIDDEN_SIZE)\n",
        "if USE_CUDA:\n",
        "    mysavedmodel = mysavedmodel.to(device)\n",
        "mysavedmodel.load_state_dict(torch.load(\"lm.pth\"))\n",
        "\n",
        "testloss = evaluate(mysavedmodel,test_iter)\n",
        "print(testloss)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.525225639343262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMor9ssj7G-V",
        "colab_type": "text"
      },
      "source": [
        "## 生成句子\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d15me0pZ7J8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49bd6b9e-0be0-49a5-cb85-5ade6c0d7c77"
      },
      "source": [
        "hidden = mysavedmodel.init_hidden(1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "words = [\"he\"]\n",
        "wordid = TEXT.vocab.stoi[words[0]]\n",
        "input = torch.tensor(wordid).long().view(1,1).to(device) #model的forward输入是【seqlenth，batchsize】\n",
        "# 先随机生成一个【1，1】大小的随机数，最大为vocabsize\n",
        "# input = torch.randint(len(TEXT.vocab), (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    output,hidden = mysavedmodel(input,hidden)\n",
        "    word_weights = output.squeeze().exp().cpu()# squeeze，去掉维度为1的\n",
        "    word_id = torch.multinomial(word_weights,1)[0] #选择概率最大的word\n",
        "    # print(input)\n",
        "    input.fill_(word_id)#TODO:why fill not concat\n",
        "    # print(input)\n",
        "    word = TEXT.vocab.itos[word_id]\n",
        "    words.append(word)\n",
        "\n",
        "print(\" \".join(words))\n",
        "\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he says that the conservative he is incapable of both special\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rF_1I3sB0Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "886de483-0304-4221-bec3-a0a61fed9497"
      },
      "source": [
        "hidden = mysavedmodel.init_hidden(1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "words = [\"he\"]\n",
        "wordslist = [TEXT.vocab.stoi[words[0]]]\n",
        "input = torch.tensor(wordlist[0]).long().view(1,1).to(device) #model的forward输入是【seqlenth，batchsize】\n",
        "\n",
        "for i in range(1,15):\n",
        "    output,hidden = mysavedmodel(input,hidden)\n",
        "    word_weights = output.squeeze().exp().cpu()\n",
        "    word_id = torch.multinomial(word_weights,1)[0] #选择概率最大的word\n",
        "    wordslist.append(word_id)\n",
        "    input = torch.tensor(wordslist).long().view(i+1,1).to(device)\n",
        "    word = TEXT.vocab.itos[word_id]\n",
        "    words.append(word)\n",
        "\n",
        "print(\" \".join(words))\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he was died placed became died in produced married later little had was one founded\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}