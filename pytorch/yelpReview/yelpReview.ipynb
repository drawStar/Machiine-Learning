{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train    39200\n",
      "val       8400\n",
      "test      8400\n",
      "Name: split, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "从完整数据集中选择一部分作为子集，分割train，val，test\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    raw_train_dataset_csv=\"data/yelp/raw_train.csv\",\n",
    "    raw_test_dataset_csv=\"data/yelp/raw_test.csv\",\n",
    "    proportion_subset_of_train=0.1,# 子集所占比例\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    output_munged_csv=\"data/yelp/reviews_with_splits_lite.csv\",\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "#读取train数据集\n",
    "train_reviews = pd.read_csv(args.raw_train_dataset_csv, header=None, names=['rating', 'review'])\n",
    "#rating 和 review分别为读取的两列内容创建的名称\n",
    "\n",
    "by_rating = collections.defaultdict(list) #dict类,value类型是list\n",
    "for _, row in train_reviews.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "    # row.to_dict()将其转化为字典｛rating：1， review：xxx｝\n",
    "\n",
    "review_subset = [] # TODO 子集的作用？\n",
    "for _, item_list in sorted(by_rating.items()): #两次for循环，第一次读取#rating为1的评论个数\n",
    "    n_total = len(item_list)\n",
    "    n_subset = int(args.proportion_subset_of_train * n_total)\n",
    "    review_subset.extend(item_list[:n_subset])\n",
    "review_subset = pd.DataFrame(review_subset)\n",
    "\n",
    "\n",
    "# 划分train，val，test。\n",
    "# 随机打乱，前0:n_train个训练，[n_train:n_train+n_val]做val集,[n_train+n_val:n_train+n_val+n_test]做test\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _,row in review_subset.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "\n",
    "final_list =[]\n",
    "np.random.seed(args.seed)\n",
    "for _,item_list in sorted(by_rating.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    n_total = len(item_list)\n",
    "    n_train = int(args.train_proportion * n_total)\n",
    "    n_val = int(args.val_proportion * n_total)\n",
    "    n_test = int(args.test_proportion * n_total)\n",
    "\n",
    "    for item in item_list[:n_train]:#给每个数据后加入train/val/test\n",
    "        item['split'] = 'train' #'rating': 1, 'review': 'xxx.', 'split': 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
    "        item['split'] = 'test'\n",
    "    final_list.extend(item_list)\n",
    "final_reviews = pd.DataFrame(final_list)\n",
    "print(final_reviews.split.value_counts()) #显示train，val，test数目\n",
    "\n",
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text) #在标点符号两边加空格\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)#将不是字母和标点的字符用空格替换\n",
    "    return text\n",
    "\n",
    "final_reviews.review = final_reviews.review.apply(preprocess_text)\n",
    "final_reviews['rating'] = final_reviews.rating.apply({1: 'negative', 2: 'positive'}.get)\n",
    "#将rating的1 2替换为neg和pos\n",
    "\n",
    "final_reviews.to_csv(args.output_munged_csv, index=False)#保存文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  terrible place to work for i just heard a stor...  train\n",
       "1  negative   hours , minutes total time for an extremely s...  train\n",
       "2  negative  my less than stellar review is for service . w...  train\n",
       "3  negative  i m granting one star because there s no way t...  train\n",
       "4  negative  the food here is mediocre at best . i went aft...  train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reviews.head()#展示前几条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"分类模型的构建和数据集处理\"\"\"\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"word<->indexs\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary unk指的是不存在于字典中的字符\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.unk_index = -1\n",
    "        # -1表示UNK未添加到Vocabulary\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        # 增加token，更新token_to_idx,idx_to_token\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        # 返回token的index,若token不存在，返回UNK index\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)  # dict.get()\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def to_serializable(self): # 用于保存Vocabulary\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    def __str__(self): # TODO:\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        instantiates the Vocabulary from a serialized dictionary\n",
    "        :param contents:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "        #**表示传入的参数为dict形式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReviewVectorizer(object):\n",
    "    \"\"\" 将word转为向量\"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): maps words to integers\n",
    "            rating_vocab (Vocabulary): maps class labels to integers\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\"return a collapsed one-hot vector representation for this review\n",
    "        Args:\n",
    "            review (str): the review\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff= 25):\n",
    "        #将出现次数ctoff以上的word，添加到V中\n",
    "        \"\"\"\n",
    "        :param review_df: (pandas.DataFrame格式) the review dataset\n",
    "        :param cutoff: word出现次数的限制，比如选出现20次以上的单词\n",
    "        :return:ReivewVectorizer的对象\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)#创建Vocabulary对象\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        #加入rating的token，即postive和negtive\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        #统计review的每个word出现的次数,将>cutoff的加入token\n",
    "        word_counts = Counter()# ｛word：count｝\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        for word, count in word_counts.items():\n",
    "            if count> cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab,rating_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        \"\"\"从一个序列化字典中实例化ReviewVectorizer\n",
    "        Instantiate a ReviewVectorizer from a serializable dictionary\n",
    "        :param contents(dict):\n",
    "        :return:  an instance of the ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        # contents是sict，用于实例化Vocabulary\n",
    "        return cls(review_vocab=review_vocab,rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Create the serializable dictionary for caching\n",
    "        :return: contents (dict): the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "继承自Dataset类，需重写__len__ 使得len(dataset)返回数据集的大小；\n",
    "和 __getitem__ 使得dataset[i]能够返回第i个数据样本这样的下标操作。\n",
    "\"\"\"\n",
    "\n",
    "class ReviewDataset(Dataset):  # 继承自Dataset类\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        :param review_df: (pandas.DataFrame), the dataset\n",
    "        :param vectorizer: (ReviewVectorizer), vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df  # review 数据集\n",
    "        self._vectorizer = vectorizer  # ReviewVectorizer类的对象\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.review_df[self.review_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_df = self.review_df[self.review_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train') #选择train，val还是test\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        the primary entry point method for PyTorch datasets\n",
    "        :param index:  the index to the data point\n",
    "        :return: a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self.target_df.iloc[index]\n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector, 'y_target':rating_index}\n",
    "\n",
    "    def get_num_batches(self,batch_size):\n",
    "        return len(self) // batch_size #//向下取整\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"\n",
    "        Load train dataset and make a new vectorizer from scratch\n",
    "        :param review_csv: location of dataset\n",
    "        :return:an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df=review_df, vectorizer=ReviewVectorizer.from_dataframe(train_review_df,args.frequency_cutoff) )\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls,review_csv,vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        Load dataset and the corresponding vectorizer.\n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        :param review_csv(str): location of the dataset\n",
    "        :param vectorizer_filepath(str):location of the saved vectorizer\n",
    "        :return: an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vecotrizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vecotrizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        a static method for loading the vectorizer from file\n",
    "        :param vectorizer_filepath(str):\n",
    "        :return: an instance of ReviewVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: ReviewClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "应用sigmoid再使用BceLoss(torch.nn.BCELoss())可能存在数值稳定性问题。\n",
    "针对此问题，Pytorch提供BCEWithLogitsLoss()，要使用此函数，输出不能使用sigmoid函数\n",
    "\"\"\"\n",
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        :param num_features: the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fcl = nn.Linear(in_features=num_features, out_features=1)\n",
    "        #使用感知器线性模型Wx+b\n",
    "        # TODO: in_feature是dim？即V的大小，onehot的长度\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid = False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor.\n",
    "                x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns: the resulting tensor. tensor.shape should be (batch,)\n",
    "        \"\"\"\n",
    "        y_out = self.fcl(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpfer funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                shuffle=shuffle, drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            #data_dict为｛x_data:batch_size个样本的collapsed one hot表示，y_target：batch_size个样本的label｝\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                #name是x_data,tensor是batch_size个样本的collapsed one hot表示\n",
    "                #name是y_target，tensor是batch_size个样本的label(即0和1)\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict\n",
    "        #TODO: for循环的作用？\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,  #效果变差已经持续的epoch数\n",
    "            'early_stopping_best_val': 1e8, #目前最好的loss\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns: a new train_state\n",
    "    \"\"\"\n",
    "    # Save one model at least 保存初始模型\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "        #选择最近两次的epoch结果\n",
    "        # If loss worsened 效果变差\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        else: # Loss decreased 效果变好\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "        # stop early?\n",
    "        train_state['stop_early']=train_state['early_stopping_step']>= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    #sigmoid（ypred）>0.5的为1类，<0.5为0类，再统计与target有相同label的\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    #item()指的是将tensor（[10]）转化为标量10\n",
    "    return n_correct/len(y_pred_indices)*100\n",
    "\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch3/yelp/vectorizer.json\n",
      "\tmodel_storage/ch3/yelp/model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',  # 不需要提前创建，handle_dirs()会创建\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    # No Model hyper parameters\n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,#可以忍受的效果变差持续的epoch数\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,  # TODO: ?\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,  # 是否从文件中加载Vectorizer\n",
    ")\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "# check cuda\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n"
     ]
    }
   ],
   "source": [
    "# dataset and  vectorizer\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading dataset and vectorizer\")\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv, args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# model\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f996869b33e44cb2aac9d6d2383655fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='trainging routine', style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15a65c1c958485ca17517af84cacd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=train', max=306, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49108e0a92f4a3d8b6500dfad7367be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='split=val', max=65, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "epoch =  1\n",
      "epoch =  2\n",
      "epoch =  3\n",
      "epoch =  4\n",
      "epoch =  5\n",
      "epoch =  6\n",
      "epoch =  7\n",
      "epoch =  8\n",
      "epoch =  9\n",
      "epoch =  10\n",
      "epoch =  11\n",
      "epoch =  12\n",
      "epoch =  13\n",
      "epoch =  14\n",
      "epoch =  15\n",
      "epoch =  16\n",
      "epoch =  17\n",
      "epoch =  18\n",
      "epoch =  19\n",
      "epoch =  20\n",
      "epoch =  21\n",
      "epoch =  22\n",
      "epoch =  23\n",
      "epoch =  24\n",
      "epoch =  25\n",
      "epoch =  26\n",
      "epoch =  27\n",
      "epoch =  28\n",
      "epoch =  29\n",
      "epoch =  30\n",
      "epoch =  31\n",
      "epoch =  32\n",
      "epoch =  33\n",
      "epoch =  34\n",
      "epoch =  35\n",
      "epoch =  36\n",
      "epoch =  37\n",
      "epoch =  38\n",
      "epoch =  39\n",
      "epoch =  40\n",
      "epoch =  41\n",
      "epoch =  42\n",
      "epoch =  43\n",
      "epoch =  44\n",
      "epoch =  45\n",
      "epoch =  46\n",
      "epoch =  47\n",
      "epoch =  48\n",
      "epoch =  49\n",
      "epoch =  50\n",
      "epoch =  51\n",
      "epoch =  52\n",
      "epoch =  53\n",
      "epoch =  54\n",
      "epoch =  55\n",
      "epoch =  56\n",
      "epoch =  57\n",
      "epoch =  58\n",
      "epoch =  59\n",
      "epoch =  60\n",
      "epoch =  61\n",
      "epoch =  62\n",
      "epoch =  63\n",
      "epoch =  64\n",
      "epoch =  65\n",
      "epoch =  66\n",
      "epoch =  67\n",
      "epoch =  68\n",
      "epoch =  69\n",
      "epoch =  70\n",
      "epoch =  71\n",
      "epoch =  72\n",
      "epoch =  73\n",
      "epoch =  74\n",
      "epoch =  75\n",
      "epoch =  76\n",
      "epoch =  77\n",
      "epoch =  78\n",
      "epoch =  79\n",
      "epoch =  80\n",
      "epoch =  81\n",
      "epoch =  82\n",
      "epoch =  83\n",
      "epoch =  84\n",
      "epoch =  85\n",
      "epoch =  86\n",
      "epoch =  87\n",
      "epoch =  88\n",
      "epoch =  89\n",
      "epoch =  90\n",
      "epoch =  91\n",
      "epoch =  92\n",
      "epoch =  93\n",
      "epoch =  94\n",
      "epoch =  95\n",
      "epoch =  96\n",
      "epoch =  97\n",
      "epoch =  98\n",
      "epoch =  99\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "# loss and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "# 调整学习率\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='trainging routine', total=args.num_epochs, position=0)\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train', total=dataset.get_num_batches(args.batch_size),\n",
    "                          position=1, leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val', total=dataset.get_num_batches(args.batch_size),\n",
    "                        position=1, leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        print(\"epoch = \",epoch_index)\n",
    "\n",
    "        # 在train集迭代\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0 #每个batch的平均loss\n",
    "        running_acc = 0.0#每个batch的平均acc\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            # TODO: xdata和ytarget是什么数据？\n",
    "            #x_data不应该是collapsed one-hot吗，为什么没用到vectorizer?\n",
    "            #答：ReviewDataset的__getitem__函数，将DataFrame格式的vectorize了\n",
    "            #ytarget是每个样本的label(0,1),pytorch会自动转化为one-hot表示\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t -running_acc) / (batch_index + 1)\n",
    "\n",
    "             # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # 在val集迭代\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.137\n",
      "Test Accuracy: 95.11\n"
     ]
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # step 2. compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "    # step 3. compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface rating of a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Ingerence rating\"\"\"\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict the rating of a review\n",
    "    :param review(str): the text of the review\n",
    "    :param classifier(ReviewClassifier): the trained model\n",
    "    :param vectorizer(ReviewVectorizer): the corresponding vectorizer\n",
    "    :param decision_threshold(float): The numerical boundary which separates the rating classes\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result = classifier(vectorized_review.view(1, -1))#reshape成1行n列\n",
    "    probability_value = torch.sigmoid(result).item() #计算sigmoid\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "    return vectorizer.rating_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a pretty awesome book -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is a pretty awesome book\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7326])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fcl.weight.shape #7326是V的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6036,  1.4505,  1.4117,  ..., -1.7685, -1.8328, -2.0215])\n",
      "tensor([2406, 1098, 7237,  ...,  890,  186,  408])\n",
      "Influential words in Positive Reviews: Top 10\n",
      "--------------------------------------\n",
      "delicious\n",
      "fantastic\n",
      "pleasantly\n",
      "amazing\n",
      "great\n",
      "vegas\n",
      "yum\n",
      "excellent\n",
      "perfect\n",
      "awesome\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "Influential words in Negative Reviews: Top 10\n",
      "--------------------------------------\n",
      "worst\n",
      "mediocre\n",
      "bland\n",
      "horrible\n",
      "meh\n",
      "awful\n",
      "rude\n",
      "terrible\n",
      "tasteless\n",
      "overpriced\n"
     ]
    }
   ],
   "source": [
    "# Sort weights\n",
    "fcl_weights = classifier.fcl.weight.detach()[0]\n",
    "i, indices = torch.sort(fcl_weights, dim=0, descending=True)\n",
    "# torch.sort() 返回排序后元素和其初始index\n",
    "print(i)\n",
    "print(indices)\n",
    "\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# Top 10 words\n",
    "print(\"Influential words in Positive Reviews: Top 10\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# Top 10 negative words\n",
    "print(\"Influential words in Negative Reviews: Top 10\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(10):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
